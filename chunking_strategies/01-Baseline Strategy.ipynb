{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Baseline Strategy\n",
    "\n",
    "Chunking documents based on different strategies will result in significant differences in search relevance. These differences vary based on the length of the documents being chunked. With the baseline strategy we will split documents into discrete chunks of length 400 words. We expect to get results that are reasonable.\n",
    "\n",
    "For more information on this experiment and it's out\n",
    "\n",
    "## RAGE-1: RAG Experiment \n",
    "\n",
    "### Setting up the Experiment\n",
    "\n",
    "Having ran [00-Chunking Strategies](./00-Chunking%20Strategies.ipynb) we now have our base data and our evaluation dataset. Now it's time to run our first experiment!\n",
    "\n",
    "Given a dataframe containing our chunked documents we must:\n",
    "\n",
    "- Chunk the documents\n",
    "- Embed the chunks.\n",
    "- Store it in a vector database.\n",
    "- Query the db using our ground truth (GT) question answer pairs.\n",
    "- Create a generation prompt which includes the question, and the retrieved \"context\".\n",
    "- Run the generation prompt and store the Question, Answer, GT Answer, Context for our evaluation framework.\n",
    "\n",
    "Let's follow our [baseline experiment](experiments/01-chunking-strategies-baseline.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiment_name = \"baseline_pubmed_articles\"\n",
    "evaluation_data = pd.read_csv('data/qa_pairs.csv')\n",
    "input_data = pd.read_csv('data/ds_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking the documents\n",
    "\n",
    "Based on our earlier analysis, let's take a chunk size of 400 words with an overlap of 50. We can calculate the exact length of requests later, but 400 words is approximately 500 tokens per chunk based on our analysis and we should be able to fit multiple results into our context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.chunking import chunk_string_with_overlap\n",
    "\n",
    "# Create a new DataFrame with each chunk as a separate row\n",
    "chunks = []\n",
    "doc_ids = []\n",
    "chunk_ids = []\n",
    "for idx, row in input_data.iterrows():\n",
    "    article_chunks = chunk_string_with_overlap(input_text=row['article'], chunk_length=400, overlap=50)\n",
    "    chunks.extend(article_chunks)\n",
    "    doc_ids.extend([row['doc_id']] * len(article_chunks))\n",
    "    chunk_ids.extend([f\"{row['doc_id']}-{i+1}\" for i in range(len(article_chunks))])\n",
    "\n",
    "ds_chunked = pd.DataFrame({'doc_id': doc_ids, 'chunk_id': chunk_ids, 'chunks': chunks})\n",
    "ds_chunked.to_csv('data/ds_chunked.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of our chunks - it helps to do a sense check as if we see something wrong, it's a lot easier to fix now than after creating your embeddings and search indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a random sample of the chunked data show the full string\n",
    "for chunk in ds_chunked['chunks'].sample(5):\n",
    "    print(chunk)\n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we can see that sentences are broken, and that perhaps this isn't a great way of splitting our information. That being said we are generating a baseline and we expect the subsequent experiments to offer significant uplift. For now let's proceed.\n",
    "\n",
    "> KEY TAKEAWAY: Even before we've run expensive and time consuming API calls, we can see that the results are not ideal. It's often worth iterating on this before investing time and money in the more nuanced tuning approaches.\n",
    "\n",
    "#### Embed the chunks\n",
    "\n",
    "We will use the `ada-v2` embedding model for this example as it is fairly powerful and well understood. It's worth noting that this will not always be the best model, paticularly when data contains topics and content that relate to finding outside of the embedding model's training data. Fine tuning an embedding model on a specific corpus (particularly in the case of highly specialised data) is also a popular option.\n",
    "\n",
    "Most vector databases implement a wrapper around common embedding functions. Here we will configure and use the wrapper for openai embeddings in ChromaDB. This function is used to embed all documents in a collection, and also to embed queries as they come in.\n",
    "\n",
    "In creating the index, we also need to specify the measurement method. For illustratice purposes we've used cosine similarity. In reality, for enterprise use cases index design is a rich topic in itself. Again, call out if you'd like more content on index design and choice of search engine! \n",
    "\n",
    "> KEY TAKEAWAY: Your choice of embedding model matters! It should be consistent across your index. and should be relevant to your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())   \n",
    "# Specify Embedding model\n",
    "embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "# Used in chromadb to embed docs and queries\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                api_type=\"azure\",\n",
    "                api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                model_name=embedding_model\n",
    "            )\n",
    "# Create a new collection in ChromaDB\n",
    "\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "chroma_client = PersistentClient(path=\"./data/chroma_db\")\n",
    "\n",
    "index_name = f\"experiment_{experiment_name}\"\n",
    "collection = chroma_client.get_or_create_collection(name=index_name,embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "collection.add(\n",
    "    # embeddings=ds_chunked['ada_v2'].tolist(),\n",
    "    documents=ds_chunked['chunks'].tolist(),\n",
    "    metadatas=[{\"doc_id\": doc_id} for doc_id in ds_chunked['doc_id']],\n",
    "    ids=ds_chunked['chunk_id'].tolist()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a quick look at an example question and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[evaluation_data['question'][3]],\n",
    "    n_results=5)\n",
    "\n",
    "print(evaluation_data['question'][3])\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the query returns the ID's, scores (distances), metadata and documents (chunks) for the top 5 documents in the collection when scored by cosine similarity. The chunks will form our context, and we will use the metadata for lineage. In our case, we see that there are a number of chunks from the same document - this can be seen as a positive indicator given our corpus is quite specific and documents can be distinct. \n",
    "\n",
    "> NOTE: It will depend on your use case and data whether there is a concept of \"the right doc\"\n",
    "\n",
    "Now let's take a look at our augmentation and generation steps and apply this at scale!\n",
    "\n",
    "#### Augmentation and Generation\n",
    "\n",
    "Here we are enriching the question with the new (and hopefully relevant!) context we have unearthed from the vector database. To do that, we'll need another prompt template. Once we have this, we can submit the prompt to our generation model and receive the answer to our question.\n",
    "\n",
    "Let's use the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.augmentation import get_context, contruct_prompt\n",
    "\n",
    "context = get_context(evaluation_data['question'][3], collection, 3)\n",
    "prompt = contruct_prompt(context, evaluation_data['question'][3])\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we've created a new prompt that's ready to be submited to our generation model. Let's take a look at a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.openai_utils import general_prompt, create_client\n",
    "\n",
    "oai_client = create_client()\n",
    "\n",
    "response = general_prompt(oai_client, prompt, model=os.getenv(\"GEN_STEP_MODEL\"))\n",
    "\n",
    "print(f\"Question: {evaluation_data['question'][3]}\")\n",
    "print(f\"Correct Answer: {evaluation_data['ground_truth'][3]}\")\n",
    "\n",
    "print(f\"Generated Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking pretty good! What you've likely just experienced is the infamous \"vibe check\" for LLM based applications. We'll get on to more formal measurement soon. But first, let's get answers to all 250 questions.\n",
    "\n",
    "> NOTE: The execution time of this will heavily depend on your model selection. For GPT-36-turbo-16k it should complete in roughly 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "model = os.getenv(\"GEN_STEP_MODEL\")\n",
    "multi_threading = True\n",
    "\n",
    "#Create a distinct copy of evaluation_data to store the results\n",
    "results_df = evaluation_data.copy()\n",
    "\n",
    "if os.path.exists(f'data/results-{experiment_name}-{model}.csv'):\n",
    "    print(\"File exists, reading in...\")\n",
    "\n",
    "    results_df = pd.read_csv(f'data/results-{experiment_name}-{model}.csv')\n",
    "\n",
    "else:\n",
    "    def generation_step(question):\n",
    "        context = get_context(question, collection,3)\n",
    "        prompt = contruct_prompt(context, question)\n",
    "        return general_prompt(oai_client, prompt, model=model)\n",
    "\n",
    "    if multi_threading == True:\n",
    "        with Pool() as pool:\n",
    "            results_multiprocessing = pool.map(generation_step, results_df['question'])\n",
    "        results_df['answer'] = results_multiprocessing\n",
    "\n",
    "    else:\n",
    "        results_df['answer'] = results_df['question'].apply(lambda x: generation_step(x))\n",
    "\n",
    "    #TODO: Refactor this so only one call for context\n",
    "\n",
    "    # Check if the column exists\n",
    "    if 'contexts' not in results_df.columns:\n",
    "        results_df['contexts'] = [get_context(q, collection) for q in results_df['question']]\n",
    "\n",
    "    #write out to CSV\n",
    "    results_df.to_csv(f'data/results-{experiment_name}-{model}.csv', index=False)\n",
    "\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe with the questions, true answers, generated answers, and the context used to generate them, we can start to look at whether or not the answers are any good. To do that, we'll use a popular open source LLM evaluation framework called [Ragas](https://docs.ragas.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGE-2: RAG Evaluation Harness\n",
    "\n",
    "Let's see how well our application performs. We'll be using the following measures to evaluate our results:\n",
    "\n",
    "1. *Faithfulness*: This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "2. *Answer Relevancy*: The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy.\n",
    "\n",
    "3. *Answer Semantic Similarity*: The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.\n",
    "\n",
    "For more information on how these are calculated you can visit the documentation [here](https://docs.ragas.io/en/stable/concepts/metrics/index.html#ragas-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.evaluate import ragas_evaluate\n",
    "\n",
    "\n",
    "#check if results_df exists, if not import it\n",
    "if 'results_df' not in locals():\n",
    "    import os\n",
    "    import ast\n",
    "    import pandas as pd\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "    experiment_name = \"baseline_pubmed_articles\"\n",
    "    model = os.getenv(\"GEN_STEP_MODEL\")\n",
    "    results_df = pd.read_csv(f'data/results-{experiment_name}-{model}.csv')\n",
    "\n",
    "    # Convert the contexts to a list of strings using ast\n",
    "    results_df['contexts'] = results_df['contexts'].apply(ast.literal_eval)\n",
    "\n",
    "# Calculate metrics and store\n",
    "results = ragas_evaluate(results_df)\n",
    "pd_results = results.to_pandas()\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(results)\n",
    "\n",
    "pd_results = results.to_pandas()\n",
    "display(pd_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
