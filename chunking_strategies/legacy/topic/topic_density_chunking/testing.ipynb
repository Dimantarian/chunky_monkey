{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.07275390625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "docs = fetch_20newsgroups(subset='all')['data']\n",
    "\n",
    "# find the average length in words of the documents\n",
    "avg_len = sum([len(doc.split()) for doc in docs]) / len(docs)\n",
    "avg_len\n",
    "\n",
    "283*5*4*384/(2**20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = sentence_model.encode(\"This is a string\", show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fa638ee26b4aa2ac4d15b90edb373e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "sentences = ['How big is London', 'How big is Paris?', 'Where is London?', 'Who is Obama?', 'Who is Trump?', 'Who is Cameron?', 'How big is England?', 'How big is France?', 'How big is England?']\n",
    "embedding = model.encode(sentences,show_progress_bar=True)\n",
    "\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Your document text. It can contain mulbtiple sentences.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/guy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-01-12 16:28:43,162 - INFO - data_processing.example - Creating example data\n",
      "2024-01-12 16:28:44,835 - INFO - data_processing.example - Created 2500 example records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a43b18549e41eab903e962a0624c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>string</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70b8a19c-6b1d-4980-8350-c5c1b0862672</td>\n",
       "      <td>ac7c5449-29ac-494e-9f97-9969579616bd</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I got the offer to upgrade this weekend.</td>\n",
       "      <td>[-0.05333279073238373, -0.005516815930604935, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70b8a19c-6b1d-4980-8350-c5c1b0862672</td>\n",
       "      <td>ac7c5449-29ac-494e-9f97-9969579616bd</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>I got the offer to upgrade this weekend. It's ...</td>\n",
       "      <td>[-0.05075643211603165, 0.015621621161699295, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70b8a19c-6b1d-4980-8350-c5c1b0862672</td>\n",
       "      <td>ac7c5449-29ac-494e-9f97-9969579616bd</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>I got the offer to upgrade this weekend. It's ...</td>\n",
       "      <td>[-0.033895041793584824, -0.01855466328561306, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70b8a19c-6b1d-4980-8350-c5c1b0862672</td>\n",
       "      <td>f127fc2a-2343-432d-8fd7-b9d1c55bc6e1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>The S+H seem way too steep for just a couple o...</td>\n",
       "      <td>[-0.0467366948723793, -0.02633601427078247, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70b8a19c-6b1d-4980-8350-c5c1b0862672</td>\n",
       "      <td>f127fc2a-2343-432d-8fd7-b9d1c55bc6e1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>The S+H seem way too steep for just a couple o...</td>\n",
       "      <td>[-0.03360332176089287, -0.015437657944858074, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39525</th>\n",
       "      <td>d5e58306-3b8a-4652-90b7-47bde010fa21</td>\n",
       "      <td>6bda2138-9fc3-4587-97ad-d03b2f91f6a6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Feng</td>\n",
       "      <td>[-0.07738784700632095, 0.06844016909599304, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39526</th>\n",
       "      <td>34c38175-aeca-458d-9582-4579089dda53</td>\n",
       "      <td>cedb6484-f1be-412a-ab56-22f5ef0bb15f</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hello out there,\\nIf your familiar with the CO...</td>\n",
       "      <td>[-0.03322175145149231, -0.050955597311258316, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39527</th>\n",
       "      <td>34c38175-aeca-458d-9582-4579089dda53</td>\n",
       "      <td>cedb6484-f1be-412a-ab56-22f5ef0bb15f</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Hello out there,\\nIf your familiar with the CO...</td>\n",
       "      <td>[0.024918068200349808, -0.03647035360336304, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39528</th>\n",
       "      <td>34c38175-aeca-458d-9582-4579089dda53</td>\n",
       "      <td>cedb6484-f1be-412a-ab56-22f5ef0bb15f</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Hello out there,\\nIf your familiar with the CO...</td>\n",
       "      <td>[-0.00042022584239020944, -0.03625414147973060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39529</th>\n",
       "      <td>34c38175-aeca-458d-9582-4579089dda53</td>\n",
       "      <td>046c77dc-ac10-43a8-aebf-a7f136b08cf6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Does anyone know if an official launch date ha...</td>\n",
       "      <td>[-0.03340361639857292, -0.046982113271951675, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39530 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0      70b8a19c-6b1d-4980-8350-c5c1b0862672   \n",
       "1      70b8a19c-6b1d-4980-8350-c5c1b0862672   \n",
       "2      70b8a19c-6b1d-4980-8350-c5c1b0862672   \n",
       "3      70b8a19c-6b1d-4980-8350-c5c1b0862672   \n",
       "4      70b8a19c-6b1d-4980-8350-c5c1b0862672   \n",
       "...                                     ...   \n",
       "39525  d5e58306-3b8a-4652-90b7-47bde010fa21   \n",
       "39526  34c38175-aeca-458d-9582-4579089dda53   \n",
       "39527  34c38175-aeca-458d-9582-4579089dda53   \n",
       "39528  34c38175-aeca-458d-9582-4579089dda53   \n",
       "39529  34c38175-aeca-458d-9582-4579089dda53   \n",
       "\n",
       "                                   chunk_id  start  end  \\\n",
       "0      ac7c5449-29ac-494e-9f97-9969579616bd      0    1   \n",
       "1      ac7c5449-29ac-494e-9f97-9969579616bd      0    2   \n",
       "2      ac7c5449-29ac-494e-9f97-9969579616bd      0    3   \n",
       "3      f127fc2a-2343-432d-8fd7-b9d1c55bc6e1      2    3   \n",
       "4      f127fc2a-2343-432d-8fd7-b9d1c55bc6e1      2    4   \n",
       "...                                     ...    ...  ...   \n",
       "39525  6bda2138-9fc3-4587-97ad-d03b2f91f6a6      3    4   \n",
       "39526  cedb6484-f1be-412a-ab56-22f5ef0bb15f      0    1   \n",
       "39527  cedb6484-f1be-412a-ab56-22f5ef0bb15f      0    2   \n",
       "39528  cedb6484-f1be-412a-ab56-22f5ef0bb15f      0    3   \n",
       "39529  046c77dc-ac10-43a8-aebf-a7f136b08cf6      2    3   \n",
       "\n",
       "                                                  string  \\\n",
       "0               I got the offer to upgrade this weekend.   \n",
       "1      I got the offer to upgrade this weekend. It's ...   \n",
       "2      I got the offer to upgrade this weekend. It's ...   \n",
       "3      The S+H seem way too steep for just a couple o...   \n",
       "4      The S+H seem way too steep for just a couple o...   \n",
       "...                                                  ...   \n",
       "39525                                               Feng   \n",
       "39526  Hello out there,\\nIf your familiar with the CO...   \n",
       "39527  Hello out there,\\nIf your familiar with the CO...   \n",
       "39528  Hello out there,\\nIf your familiar with the CO...   \n",
       "39529  Does anyone know if an official launch date ha...   \n",
       "\n",
       "                                              embeddings  \n",
       "0      [-0.05333279073238373, -0.005516815930604935, ...  \n",
       "1      [-0.05075643211603165, 0.015621621161699295, 0...  \n",
       "2      [-0.033895041793584824, -0.01855466328561306, ...  \n",
       "3      [-0.0467366948723793, -0.02633601427078247, -0...  \n",
       "4      [-0.03360332176089287, -0.015437657944858074, ...  \n",
       "...                                                  ...  \n",
       "39525  [-0.07738784700632095, 0.06844016909599304, 0....  \n",
       "39526  [-0.03322175145149231, -0.050955597311258316, ...  \n",
       "39527  [0.024918068200349808, -0.03647035360336304, 0...  \n",
       "39528  [-0.00042022584239020944, -0.03625414147973060...  \n",
       "39529  [-0.03340361639857292, -0.046982113271951675, ...  \n",
       "\n",
       "[39530 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from uuid import uuid4\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_and_reconstitute(strings, minimum, maximum, increment, overlap):\n",
    "    def split_sentences(text):\n",
    "        \"\"\"Split the given text into sentences using NLTK.\"\"\"\n",
    "        return nltk.sent_tokenize(text)\n",
    "\n",
    "    def reconstitute(sentences, min_sentences, max_sentences, increment, overlap):\n",
    "        result = []\n",
    "        start_index = 0\n",
    "        doc_id = str(uuid4())\n",
    "\n",
    "        while start_index < len(sentences):\n",
    "            end_index = start_index + min_sentences\n",
    "            chunk_id = str(uuid4())\n",
    "\n",
    "            while end_index <= len(sentences) and (end_index - start_index) <= max_sentences:\n",
    "                result.append(' '.join(sentences[start_index:end_index]))\n",
    "                yield doc_id, chunk_id, start_index, end_index, result[-1]\n",
    "                end_index += increment\n",
    "\n",
    "            next_start = end_index - overlap - 1\n",
    "            if next_start <= start_index:\n",
    "                break\n",
    "\n",
    "            start_index = next_start\n",
    "\n",
    "            if end_index >= len(sentences) and (end_index - start_index) <= min_sentences:\n",
    "                break\n",
    "\n",
    "        return result\n",
    "\n",
    "    data = []\n",
    "    for string in strings:\n",
    "        sentences = split_sentences(string)\n",
    "        for item in reconstitute(sentences, minimum, maximum, increment, overlap):\n",
    "            data.append(item)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['doc_id', 'chunk_id', 'start', 'end', 'string'])\n",
    "    return df\n",
    "\n",
    "input_strings = [\"This is a test. Testing sentence splitting. Another sentence here. This is another string. It has two sentences. This is a third string. It has three sentences.\",\n",
    "                 \"This is a test. Testing sentence splitting. Another sentence here. This is another string. It has two sentences. This is a third string. It has three sentences.\"]\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from data_processing.example import create_example_input_docs\n",
    "\n",
    "\n",
    "# Restrict to 2500 documents for testing\n",
    "docs = create_example_input_docs(2500)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "result = split_and_reconstitute(docs, 1, 3, 1, 1)\n",
    "result.shape\n",
    "\n",
    "#350 seems to be a sweet spot\n",
    "result['embeddings'] = model.encode(result['string'].tolist(), show_progress_bar=True,batch_size=250, device=\"cuda\").tolist()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
