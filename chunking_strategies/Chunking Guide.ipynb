{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Guidance for Retireval Augmentation Generation \n",
    "\n",
    "Retrieval Augementation and Generation (RAG) systems are increasingly popular as a way to bootstrap an LLM on your own information. \n",
    "\n",
    "Generally it is easy to implement a proof of concept that passes the infamous \"vibe\" check fairly quickly, whilst this might get an engineer 70-80% of the way, as with any data driven system, the devil really is in the details when it comes to squeezing out the extra performance that gives end users confidence.\n",
    "\n",
    "Through this notebook we'll look at how to best understand a corpus of documents, some analysis that can help determine some of the parameters of your chunking, and how to go about running experiments to see what really works for your data. We'll run the baseline experiment \"from first principles\" i.e. we won't use any fancy tooling that abstracts what's going on under the hood. For subsequent experiments, we'll use separate notebooks and show how some popular tooling can make life a lot easier!\n",
    "\n",
    "Naturally, RAG systems have a lot of components, where possible we'll introduce core concepts, but we won't deep dive on everything here.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook aims to give direction to software engineers in how they might approach evaluating, and selecting a chunking methodology for a RAG system. It is not meant to be an accelerator or template. There are other great resources for that like the [RAG Experiment Accelerator](https://github.com/microsoft/rag-experiment-accelerator). Instead this is meant to be complementary to those assets, highlight the thinking and decisions that you might make whilst approaching a chunking problem.\n",
    "\n",
    "This notebook does NOT intend on covering end to end evaluation of RAG systems, as that is a much broader topic that dives into information retrieval. Resources for tuning those elements of a RAG system can be found [here]().\n",
    "\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. [Exploring your data](#exploring-your-data)\n",
    "2. [Chunks, Words, Tokens and Contexts](#chunks-words-token-and-contexts)\n",
    "3. [Experiment Setup](#experiment-setup)\n",
    "4. [Building an Evaluation Dataset](#building-an-evaluation-dataset)\n",
    "5. [RAGE-1: RAG Experiment Harness](#rage-1-rag-experiment-harness)\n",
    "6. [RAGE-2: RAG Evaluation Harness](#rage-2-rag-evaluation-harness)\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Loading a Corpus\n",
    "\n",
    "First things first, let's load some text data to work with. Let's go with the [pubmed summarisation dataset](https://huggingface.co/datasets/ccdv/pubmed-summarization). We'll download from hugging face, but for simplicity we'll convert the dataset to pandas, which most data pro's are familiar with. To make sure no information is lost, we'll do a quick record check count and verify that the descriptive statistics are equal for the aticle lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from datasets import load_dataset\n",
    "from uuid import uuid4\n",
    "from pprint import pprint\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tiktoken as tk\n",
    "import random\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Set to pubmed or arxiv\n",
    "publication = 'pubmed'\n",
    "\n",
    "dataset = load_dataset(f'scientific_papers',publication,split='train', trust_remote_code=True)\n",
    "\n",
    "# Convert to a pandas dataframe and do some housekeeping\n",
    "ds = dataset.to_pandas()\n",
    "ds['doc_id'] = [str(uuid4()) for _ in range(len(ds))]\n",
    "ds['article_len'] = ds['article'].apply(lambda x: len(x.split()))\n",
    "\n",
    "ds.drop(columns=['abstract'],inplace=True)\n",
    "\n",
    "# Take a look at our data\n",
    "display(ds.head())\n",
    "\n",
    "# Check to make sure that the conversion hasn't caused any issues\n",
    "\n",
    "# Number of records\n",
    "print(\"Number of records in original dataset: \", len(dataset))\n",
    "print(\"Number of records in transformed dataset: \", ds.shape[0])\n",
    "\n",
    "# Descriptive statistics for article length\n",
    "original_lengths = [len(article.split()) for article in dataset['article']]\n",
    "\n",
    "if pd.Series(original_lengths).describe().equals(ds['article_len'].describe()):\n",
    "    print(\"\\nArticle length statistics match!\")\n",
    "\n",
    "    print(\"\\nDescriptive statistics for article length in original dataset:\")\n",
    "    display(pd.Series(original_lengths).describe())\n",
    "\n",
    "    print(\"\\nDescriptive statistics for article length in transformed dataset:\")\n",
    "    display(ds['article_len'].describe())\n",
    "\n",
    "else:\n",
    "    print(\"\\nArticle length statistics do not match! Please check the conversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring your data\n",
    "\n",
    "One of the main things that we are concerned with when building RAG systems is the length of the data that we intend to feed in to an LLM. Even with context windows (i.e. the length of the input + output to/from an LLM) are becoming increasingly large, but we still don't really know if simply throwing more data at an LLM effective, and it is definitely not quick.\n",
    "\n",
    "Let's start by understanding just how long a typical article is, and get a feel for the boundaries and overall distribution of our data.\n",
    "\n",
    "An easy way to do this is to look at some basic stats, and draw a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds.describe())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ds['article_len'], bins=50, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the table and the diagram show what's called a heavy right skew - meaning that there are a small number of extremely large values. \n",
    "\n",
    "What we do with these extra long articles is an important decision. For some data problems you'd look to exclude outliers. But given this is an information retrieval problem, we'd like to keep as much data as possible. \n",
    "\n",
    "What we need to do, is understand  *why* some of these articles are so long, and deal with them accordingly\n",
    "\n",
    "First, lets zoom in on the distribution and if we can find a more reasonable cut off point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.describe(percentiles=[0.75,0.8, 0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is definitely in the high end of town, with a jump from 10k to 112k in number of words for the last percentile. Let's take a closer look at the raw data and see if we can tell what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the articles with the longest length\n",
    "longest_articles = ds['article_len'].nlargest(5).index\n",
    "for idx in longest_articles:\n",
    "    print(f'Article Length: {ds[\"article_len\"][idx]}\\n')\n",
    "    print(ds['article'][idx]+'\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it appears that the two main causes of long documents are either:\n",
    "- LaTeX package inclusions (mathematical formatting for scientific documents)\n",
    "- Data tables (from pharmaceutical research by the look of it)\n",
    "\n",
    "In practice we would want to spend more time understanding the drivers behind the outliers, and address as many as possible. \n",
    "\n",
    "However, for the purposes of this exercise we will focus on the LaTeX issue. \n",
    "\n",
    "Data tables could be solved through an application of difference pdf cracking techniques (e.g. [Azure Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-retrieval-augmented-generation?view=doc-intel-4.0.0)), but that is out of scope for this notebook.\n",
    "\n",
    "Let's remove the the lines which include LaTeX and take another look at the adjusted distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.general import remove_latex_packages\n",
    "\n",
    "# Remove LaTeX package inclusions from the articles\n",
    "ds['article'] = ds['article'].apply(remove_latex_packages)\n",
    "\n",
    "# Recalculate article lengths\n",
    "ds['article_len'] = ds['article'].apply(lambda x: len(x.split()))\n",
    "\n",
    "display(ds.describe(percentiles=[0.75,0.8, 0.9,0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made a difference, but 95k is still very large. For now, let's exclude the longer documents, storing them in another dataframe for analysis later.\n",
    "\n",
    "Once we remove the odd docs, we'll check our distribution again to make sure that we now have something workable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.general import remove_over_percentile\n",
    "\n",
    "#apply helper function from utils module\n",
    "ds_99pct, ds_outliers = remove_over_percentile(ds, 'article_len', .99)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the first histogram on the first subplot\n",
    "sns.histplot(ds_99pct['article_len'], bins=100, kde=True)\n",
    "plt.title('Article Length')\n",
    "\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n",
    "display(ds_99pct.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the smallest articles too. The above histogram indicates that there are a number of articles that have length of less than 500 words. Let's check formally and then take a peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tiny_articles = ds_99pct['article_len'][ds_99pct['article_len'] < 500].count()\n",
    "\n",
    "print(f'Number of articles with less than 500 words: {count_tiny_articles}')\n",
    "\n",
    "smallest_articles = ds_99pct['article_len'].nsmallest(count_tiny_articles).index\n",
    "for idx in smallest_articles:\n",
    "    print(f'Article Length: {ds_99pct[\"article_len\"][idx]}\\n')\n",
    "    print(ds_99pct['article'][idx]+'\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanning through the articles there are some empty records, but the majority seem to be fractions of documents rather than complete articles. We can be confident that this isn't an issue with the hugging face dataset conversion as we tested that at the start of the notebook. We can reasonably conclude that these are invalid records and exclude them from our system. It's worth calling out that this exclusion is different from excluding the large ones, as the are malformed at source and there is little that we can do to remediate without calling a friend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean = ds_99pct[ds_99pct['article_len'] >= 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cleansed dataset, let's start talking about tokens, why it's good to know about them, and how they relate to designing a RAG system.\n",
    "\n",
    "## Chunks, Words, Token and Contexts\n",
    "\n",
    "Despite feeling like LLMs converse in our language, there's a few things that go in behind the scenes that translate our verbiage into something an algorithm understands. \n",
    "\n",
    "Firstly, the text is `tokenized`, which means words are split into a list of `tokens`. Think of this a bit like stemming in NLP. For shorter words, the ratio of tokens to words can be 1:1 (i.e. the word = the token), but for longer, or more complex words the ratio can be far higher. \n",
    "\n",
    "These lists are then mapped into numerical vectors that the algorithm can understand. For a given corpus, we could work out the exact ratio - in fact, let's do that!\n",
    "\n",
    "(for a visual explanation of tokens see [this](https://www.tokencounter.io/) excellent resource)\n",
    "\n",
    "First we will apply the appropriate encoding to the text, then we simply sum up the number of tokens and words in the corpus and calculate the number of tokens / word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tokeniser over the articles and store the results in the DataFrame\n",
    "# Given the size of the dataet this can take up to 10 minutes Let's select a \n",
    "# random sample of 5000 articles to reduce the time down to ~ 10 seconds\n",
    "# Feel free to grab a beverage and run the analsis on the entire dataset to\n",
    "# compare the results if you have the time!\n",
    "\n",
    "subset = True\n",
    "\n",
    "if subset == True:\n",
    "    random.seed(42)\n",
    "    random_doc_ids = random.sample(list(ds_clean['doc_id'].unique()), 5000)\n",
    "\n",
    "    # Subset the DataFrames\n",
    "    ds_clean = ds_clean[ds_clean['doc_id'].isin(random_doc_ids)]\n",
    "\n",
    "encoding = tk.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "article_tokens = ds_clean['article'].apply(encoding.encode)\n",
    "\n",
    "# check if columns already exist\n",
    "if 'article_tokens' in ds_clean.columns:\n",
    "    ds_token_count_subset = ds_clean.drop(columns=['article_tokens'])\n",
    "\n",
    "ds_clean = ds_clean.assign(article_tokens=article_tokens)\n",
    "ds_clean['article_tk_len'] = ds_clean['article_tokens'].apply(lambda x: len(x))\n",
    "ds_clean['token_ratio'] = ds_clean['article_tk_len']/ds_clean['article_len']\n",
    "\n",
    "total_words = ds_clean['article_len'].sum()\n",
    "total_tokens = ds_clean['article_tk_len'].sum()\n",
    "\n",
    "ratio = total_tokens/total_words\n",
    "\n",
    "print(f'The ratio of tokens to words is {ratio:.2f}')\n",
    "print(f\"The largest article has {ds_clean['article_tk_len'].max()} tokens\")\n",
    "print(f\"The smallest article has {ds_clean['article_tk_len'].min()} tokens\")\n",
    "print(f\"The mean number of tokens is : {ds_clean['article_tk_len'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the overall ratio we can make some assumptions around cost and how much information we feed the generation step as context.\n",
    "\n",
    "Before we move on, let's just take a look at the distribution of ratios - some articles might use more complex language than others.\n",
    "\n",
    "For this we'll use a violin plot which is similar to a box plot, but can be used to give great visuals across multiple classes (e.g. pharma vs biotech articles).\n",
    "\n",
    "The plot below shows that we have a few articles that have high token ratios - an interesting problem for another time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a violin plot of token ratios\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='token_ratio', data=ds_clean)\n",
    "plt.title('Token Ratio')\n",
    "plt.show()\n",
    "\n",
    "# TODO: Wouldn't it be fun to find two classes of articles with different token ratios and compare them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care about this?\n",
    "\n",
    "How many records do we want to include in our Augmentation step when constructing the generation prompt? Say we're using GPT-35-Turbo, we have aprx 4000 tokens to play with. This is both **input and output**.\n",
    "\n",
    "Let's assume the following:\n",
    "\n",
    "1. We have a prompt template which is a total of 500 tokens, including our guardrails, instructions and any other boiler plate commentary that needs to be input to the generation step.\n",
    "2. We allow for up to 500 tokens in a response. \n",
    "\n",
    "$$Guardrails + Instructions + Estimated Response Length + Retrieved Context < Model Token Limit$$\n",
    "\n",
    "\n",
    "Meaning with **GPT-35-Turbo**:\n",
    "$$ 500 + 500 + Retrieved Context < 4000 $$\n",
    "\n",
    "or $ Retrieved Context < 3000 $\n",
    "\n",
    "This leaves us with 3000 tokens to play with. If we assume a chunk size  of 500, that gives us 6 records in our retrieval step. In fact, this might be a good starting point. It glosses over whether or not 500 tokens is enough to capture something that is semantically relevant and (more importantly!) useful for the generation step, but we'll take a look at that as we experiment with different chunking strategies.\n",
    "\n",
    "Now we have some idea of a driving non functional contraint. Let's talk about what makes a good chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes a good chunk?\n",
    "\n",
    "Let's start by really addressing what a chunk is intended to do. It's sole purpose is to provide context to a model that enhances the available \"knowledge\" available to the LLM to better answer a given question.\n",
    "\n",
    "To do that well, a chunk should:\n",
    "\n",
    "1. Be relevant to the query or task\n",
    "2. Be factually accurate and avoid misleading information\n",
    "3. Be specific - focused information rather than overly broad content\n",
    "4. Concise and information dense. Conveying information effectively within a limited space\n",
    "5. Referenceable. A user should always be able to go back and see the chunk in context of it's parent document\n",
    "\n",
    "Let's talk about each in turn:\n",
    "\n",
    "#### Relevance\n",
    "\n",
    "This is primarily the focus of optimising search results. What metrics do you choose to select you chunks when executing the retrieval step. [Cosine Similarity]() and the [dot product]() are two commonly used metrics. You may expand this by performing a hybrid search (adding metadata to your quesry - for example published dates, authors etc.), or even looking at strategies for [re-ranking](). The specifics of these techniques are beyond the scope of this notebook as they fall into a separate, and expansive discipline (Maybe another notebook!). In our example we are simply taking the top n records based on cosine similarity.\n",
    "\n",
    "#### Factual Accuracy\n",
    "\n",
    "This is fundamentally a data governance problem. Where did your data come from, is it up to date, can you trust the source. An example provided in a recent epiisode of [Practical AI]() gave the example of an HR bot being queried on annual leave policies. One data source could be the official HR policy which contains relevant, up to date information, whereas another source could be chat logs that are dated and included discussions from other georgaphies that aren't relevant to the users context. In our example, we are essentially outsourcing the governance problem by trusting the Hugging Face dataset and PubMed as sources.\n",
    "\n",
    "#### Specificity\n",
    "\n",
    "This is where our chunking strategy starts to play a part. How many topics does a chunk cover? Is it general in nature, or does it cut to the point. It may be that we find that chunk length correlates well with specificity. Perhaps we need ot look at topics, or semantic consistency in chunks. More on all of this to come!\n",
    "\n",
    "#### Concise\n",
    "\n",
    "We've illustrated the finite number of tokens we have to play with in the previous section. We ideally want information dense chunks that provide the correct context without rambling and consuming too many tokens\n",
    "\n",
    "#### Referencable\n",
    "\n",
    "Whilst the ultimate answer is generated by an LLM, often users will want to check the source. It is best practice to provide both the context, and the source of the context items back to the user for verification and / or further research. We'll achieve this by making use of metadata in our vector index.\n",
    "\n",
    "Now we understand our data, how we need to manage the inputs to a RAG system, and what a good output looks like. Let's set up our experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "\n",
    "At Microsoft, we use a framework called the [AI Garden](https://github.com/cse-labs/ai-garden/tree/main). This allows us to group related experiments into \"experiment families\" and collaborate across groups and engagements in sharing knowledge and learnings.\n",
    "\n",
    "In this repo we have the [experiments](experiments) directory which contains our expermiment family and distinct experiment plans and results. We also include Architectural Decision Records (ADRs) which include important technical decisions that impact the outcome, along with reasoning. I encourage you to check these out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data\n",
    "\n",
    "Given this is an illustration of the process and thinking, now that we have a clean dataset we're going to go one step further an subset the data down to 50 randomly selected articles - mostly because I'm terrified of my GPT4 bill! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random 50 unique doc_ids and subset ds_cleam - note we will use this again later!\n",
    "random.seed(42)\n",
    "random_doc_ids = random.sample(list(ds_clean['doc_id'].unique()), 50)\n",
    "\n",
    "# Subset the DataFrames\n",
    "ds_subset = ds_clean[ds_clean['doc_id'].isin(random_doc_ids)]\n",
    "\n",
    "#write out the dataset to a csv file\n",
    "ds_subset.to_csv('data/ds_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Evaluation Dataset\n",
    "\n",
    "Now that we have the data, let's think about how we are going to evaluate our chunking strategies. The evaluation metrics are outlined in the [experiment documentation](./experiments/00-chunking-strategies-family.md). The foundation of evaluation is a good set of question / answer pairs.\n",
    "\n",
    "Why do we do this?\n",
    "\n",
    "#### Simulating Real-World Use:\n",
    "\n",
    "- QA pairs mimic how users interact with a RAG system. Users ask questions, and the system retrieves documents and generates answers based on those documents.\n",
    "- By testing with good QA pairs, you ensure the RAG system performs well in scenarios it's designed for.\n",
    "\n",
    "#### Evaluating Retrieval Accuracy:\n",
    "\n",
    "- Good QA pairs often have answers that can be found within a relevant document. This allows you to see if the RAG system retrieves the documents that actually hold the answer.\n",
    "\n",
    "- Poor retrieval throws off the entire RAG process, so evaluating retrieval accuracy is crucial.\n",
    "\n",
    "#### Assessing Answer Quality:\n",
    "\n",
    "- QA pairs with well-defined answers provide a benchmark for the RAG system's generation capabilities\n",
    "\n",
    "- You can compare the generated answer to the actual answer in the retrieved document to see if the RAG system effectively uses the information.\n",
    "\n",
    "#### Identifying System Biases:\n",
    "\n",
    "- A diverse set of QA pairs helps identify potential biases in the RAG system.\n",
    "\n",
    "- For instance, if the system struggles with specific question types or topics, the QA pairs will reveal these weaknesses.\n",
    "\n",
    "#### Not just any QA pairs will do:\n",
    "\n",
    "- Good QA pairs should be well-formed, grammatically correct, and cover a range of difficulty levels and topics relevant to your RAG system's intended use.\n",
    "\n",
    "- In essence, good QA pairs provide a realistic testing ground to assess how well your RAG system retrieves information and generates accurate and relevant answers.\n",
    "\n",
    "Now, I don't have the time or expertise to generate a set of Q&A for these pubmed articles. A popular approach is to use a leading LLM to read the documents and create the question answer pairs for you. Ideally this (or at least a subset) would be reviewed by a selection of subject matter experts before being used as a the basis for measurement.\n",
    "\n",
    "To do this, we will create a prompt template that submits each article, and generates a set of 5 question answer pairs in a specific format. Once we have the tmeplate, we can submit the prompts to our model of choice. \n",
    "\n",
    ">Note: Given this requires a structured output, we recommend using gpt-4 for this step. There are other ways to apply structure to outputs, but for simplicity's sake we'll use a the sledgehammer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.openai_utils import general_prompt, create_client\n",
    "from helper.general import convert_to_dict\n",
    "from rag.data_prep import generate_qa_prompt\n",
    "\n",
    "# Create an OpenAI client\n",
    "oai_client = create_client()\n",
    "model = os.getenv(\"QA_MODEL\")\n",
    "\n",
    "def _process_article(article):\n",
    "    return general_prompt(oai_client, generate_qa_prompt(article), model=model)\n",
    "\n",
    "\n",
    "def create_qa_pairs(client, model, articles, save_output=True, parallel=True):\n",
    "    if parallel is True:\n",
    "        with Pool() as pool:\n",
    "            results = pool.map(_process_article, articles)\n",
    "\n",
    "    else:\n",
    "        prompts = [generate_qa_prompt(article) for article in ds_subset[\"article\"]]\n",
    "        results = [general_prompt(client, prompt, model=model) for prompt in prompts]\n",
    "\n",
    "    # Save the results to a file\n",
    "    qa_pairs = convert_to_dict(results)\n",
    "\n",
    "    # TODO: Make sure this is in the format expected by AI Studio and clean up csv\n",
    "    if save_output is True:\n",
    "        with open(\"data/qa_pairs.jsonl\", \"w\") as f:\n",
    "            json.dump(qa_pairs, f, indent=4)\n",
    "\n",
    "    questions = [pair[\"question\"] for pair in qa_pairs]\n",
    "    answers = [pair[\"answer\"] for pair in qa_pairs]\n",
    "\n",
    "    # Create a DataFrame with the questions and answers\n",
    "    qa_df = pd.DataFrame({\"question\": questions, \"answer\": answers})\n",
    "\n",
    "    return qa_df\n",
    "\n",
    "# if the data is already saved, load it\n",
    "if os.path.exists('data/qa_pairs.csv'):\n",
    "    qa_df = pd.read_csv('data/qa_pairs.csv')\n",
    "else:\n",
    "    qa_df = create_qa_pairs(oai_client, model, ds_subset[\"article\"], save_output=True, parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGE-1: RAG Experiment Harness\n",
    "\n",
    "### Setting up the Retrieval step\n",
    "\n",
    "What do we need to do next. Given a dataframe containing our chunked documents we must:\n",
    "\n",
    "- Chunk the documents\n",
    "- Embed the chunks.\n",
    "- Store it in a vector database.\n",
    "- Query the db using our ground truth (GT) questions.\n",
    "- Generate a final response.\n",
    "- Store the Question, Answer, GT Answer, Context for our evaluation framework.\n",
    "\n",
    "Let's follow our [baseline experiment](experiments/01-chunking-strategies-baseline.md) as an example. Here we create chunks with a fixed length (in words), and an overlap (also in words). This is intentionally naiive to provide a clear baseline to measure the relative improvements of subsequent experiments.\n",
    "\n",
    "#### Chunking the documents\n",
    "\n",
    "Based on our earlier analysis, let's take a chunk size of 400 words with an overlap of 50. We can calculate the exact length of requests later, but 400 words is approximately 500 tokens per chunk based on our analysis and we should be able to fit multiple results into our context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.chunking import chunk_string_with_overlap\n",
    "\n",
    "# Create a new DataFrame with each chunk as a separate row\n",
    "chunks = []\n",
    "doc_ids = []\n",
    "chunk_ids = []\n",
    "for idx, row in ds_subset.iterrows():\n",
    "    article_chunks = chunk_string_with_overlap(input_text=row['article'], chunk_length=400, overlap=50)\n",
    "    chunks.extend(article_chunks)\n",
    "    doc_ids.extend([row['doc_id']] * len(article_chunks))\n",
    "    chunk_ids.extend([f\"{row['doc_id']}-{i+1}\" for i in range(len(article_chunks))])\n",
    "\n",
    "ds_chunked = pd.DataFrame({'doc_id': doc_ids, 'chunk_id': chunk_ids, 'chunks': chunks})\n",
    "ds_chunked.to_csv('data/ds_chunked.csv', index=False)\n",
    "\n",
    "# Worl out the average number of chunks per document\n",
    "avg_chunks_per_doc = ds_chunked.groupby('doc_id').size().mean()\n",
    "print(f'Average number of chunks per document: {avg_chunks_per_doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed the chunks\n",
    "\n",
    "We will use the ada-v2 embedding model for this example as it is fairly powerful and generally understood. It's worth noting that this will not always be the best model, paticularly when data contains topics and content that relate to finding outside of the embedding model's training data. Fine tuning an embedding model on a specific corpus (particularly in the case of highly specialised data) is also a popular option.\n",
    "\n",
    "Most vector databases implement a wrapper around common embedding functions. Here we will configure and use the wrapper for openai embeddings in ChromaDB. This function is used to embed all documents in a collection, and also to embed queries as they come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())   \n",
    "# Specify Embedding model\n",
    "embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "# Used in chromadb to embed docs and queries\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                api_type=\"azure\",\n",
    "                api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                model_name=embedding_model\n",
    "            )\n",
    "# Create a new collection in ChromaDB\n",
    "\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "chroma_client = PersistentClient(path=\"./data/chroma_db\")\n",
    "\n",
    "index_name = \"baseline_pubmed_articles\"\n",
    "collection = chroma_client.get_or_create_collection(name=index_name,embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "collection.add(\n",
    "    # embeddings=ds_chunked['ada_v2'].tolist(),\n",
    "    documents=ds_chunked['chunks'].tolist(),\n",
    "    metadatas=[{\"doc_id\": doc_id} for doc_id in ds_chunked['doc_id']],\n",
    "    ids=ds_chunked['chunk_id'].tolist()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a quick look at an example question and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[qa_df['question'][3]],\n",
    "    n_results=5\n",
    ")\n",
    "pprint(qa_df['question'][3])\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the query returns the ID's, scores (distances), metadata and documents (chunks) for the top 5 documents in the collection when scored by cosine similarity. The chunks will form our context, and we will use the metadata for lineage. In our case, we see that there are a number of chunks from the same document - this can be seen as a positive indicator given \n",
    "\n",
    "Now let's take a look at our augmentation and generation steps and apply this at scale!\n",
    "\n",
    "#### Augmentation and Generation\n",
    "\n",
    "Here we are enriching the question with the new (and hopefully relevant!) context we have unearthed from the vector database. To do that, we'll need another prompt template. Once we have this, we can submit the prompt to our generation model and receive the answer to our question.\n",
    "\n",
    "Let's use the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.augmentation import get_context, contruct_prompt\n",
    "\n",
    "context = get_context(qa_df['question'][3], collection, 3)\n",
    "prompt = contruct_prompt(context, qa_df['question'][3])\n",
    "\n",
    "pprint(prompt)\n",
    "\n",
    "# Count the number of tokens in the prompt\n",
    "prompt_tokens = encoding.encode(prompt)\n",
    "print(f\"Number of tokens in the prompt: {len(prompt_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we've created a new prompt that's ready to be submited to our generation model. Let's take a look at a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = general_prompt(oai_client, prompt, model=os.getenv(\"GEN_STEP_MODEL\"))\n",
    "\n",
    "print(f\"Question: {qa_df['question'][3]}\")\n",
    "print(f\"Correct Answer: {qa_df['answer'][3]}\")\n",
    "\n",
    "print(f\"Generated Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking pretty good! What you've likely just experienced is the infamous \"vibe check\" for LLM based applications. We'll get on to more formal measurement soon. But first, let's get ansers to all 250 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-4' #os.getenv(\"GEN_STEP_MODEL\")\n",
    "multi_threading = True\n",
    "\n",
    "#Create a distinct copy of qa_df to store the results\n",
    "results_df = qa_df.copy()\n",
    "\n",
    "if os.path.exists(f'data/results-{index_name}-{model}.csv'):\n",
    "    print(\"File exists, reading in...\")\n",
    "\n",
    "    results_df = pd.read_csv(f'data/results-{index_name}-{model}.csv')\n",
    "\n",
    "else:\n",
    "    def generation_step(question):\n",
    "        context = get_context(question, collection,3)\n",
    "        prompt = contruct_prompt(context, question)\n",
    "        return general_prompt(oai_client, prompt, model=model)\n",
    "\n",
    "    if multi_threading == True:\n",
    "        with Pool() as pool:\n",
    "            results_multiprocessing = pool.map(generation_step, results_df['question'])\n",
    "        results_df['response'] = results_multiprocessing\n",
    "\n",
    "    else:\n",
    "        results_df['response'] = results_df['question'].apply(lambda x: generation_step(x))\n",
    "\n",
    "    #TODO: Refactor this so only one call for context\n",
    "    # Check if the column exists\n",
    "    if 'context' not in qa_df.columns:\n",
    "        results_df['context'] = [get_context(q, collection) for q in results_df['question']]\n",
    "\n",
    "    #write out to CSV\n",
    "    results_df.to_csv(f'data/results-{index_name}-{model}.csv', index=False)\n",
    "\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe with the questions, true answers, generated answers, and the context used to generate them, we can start to look at whether or not the answers are any good. To do that, we'll use the evaluation framework in Azure AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGE-2: RAG Evaluation Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
