{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Chunking\n",
    "\n",
    "Retrieval Augementation and Generation (RAG) systems are increasingly common. Generally it is easy to implement a proof of concept that passes the infamous \"vibe\" check fairly quickly, whilst this might get an engineer 70-80% of the way, as with any data driven system, the devil really is in the details when it comes to squeezing out the extra performance that gives end users confidence.\n",
    "\n",
    "The questions we'll look at include:\n",
    "- What length should my chunks be?\n",
    "- Should all of my chunks be the same size?\n",
    "- What's the distribution of my chunk lengths?\n",
    "- How should I consider the relationship between my chunk length, and the context window in my generation step?\n",
    "- Do my chunks make sense in the context of my business problem?\n",
    "- Is semantic purity important for my chunks? How do I measure it?\n",
    "\n",
    "This notebook aims to give direction to data professionals in how they might approach evaluating, and selecting a chunking methodology for a RAG system. \n",
    "\n",
    "This notebook does NOT intend on covering end to end evaluation of RAG systems, as that is a much broader topic that dives into information retrieval. Resources for tuning those elements of a RAG system can be found [here]().\n",
    "\n",
    "Let's load some data and get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a corpus\n",
    "First things first, let's load some text data to work with. Let's go with the [pubmed summarisation dataset](https://huggingface.co/datasets/ccdv/pubmed-summarization). We'll download from hugging face, but for simplicity we'll convert the dataset to pandas, which most data pro's are familiar with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from uuid import uuid4\n",
    "from pprint import pprint\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tiktoken as tk\n",
    "import random\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Set to pubmed or arxiv\n",
    "publication = 'pubmed'\n",
    "\n",
    "dataset = load_dataset(f'ccdv/{publication}-summarization',split='validation',trust_remote_code=True)\n",
    "\n",
    "# Convert to a pandas dataframe and do some housekeeping\n",
    "ds = dataset.to_pandas()\n",
    "ds['doc_id'] = [str(uuid4()) for _ in range(len(ds))]\n",
    "ds['article_len'] = ds['article'].apply(lambda x: len(x.split()))\n",
    "ds['abstract_len'] = ds['abstract'].apply(lambda x: len(x.split()))\n",
    "\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of article and abstract lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot a histogram with seaborn\n",
    "\n",
    "# Create a figure and a 1x2 grid of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first histogram on the first subplot\n",
    "sns.histplot(ds['article_len'], bins=50, kde=True, ax=axs[0])\n",
    "axs[0].set_title('Article Length')\n",
    "\n",
    "# Plot the second histogram on the second subplot\n",
    "sns.histplot(ds['abstract_len'], bins=50, kde=True, ax=axs[1])\n",
    "axs[1].set_title('Abstract Length')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we even start diving deeper, we can see the heavy right skew of the article length distribution. For now, let's take a closer look at the length percentiles to see what might make dor a good cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.describe(percentiles=[0.75,0.8, 0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is definitely in the high end of town, with a jump from 10k to 112k in number of words for the last percentile. Let's take a closer look at the raw data and see if we can tell what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the articles with the longest length\n",
    "longest_articles = ds['article_len'].nlargest(5).index\n",
    "for idx in longest_articles:\n",
    "    print(f'Article Length: {ds[\"article_len\"][idx]}\\n')\n",
    "    print(ds['article'][idx]+'\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it appears that the two main causes of long documents are either:\n",
    "- LaTeX package inclusions (mathematical formatting for scientific documents)\n",
    "- Data tables (from pharmaceutical research by the look of it)\n",
    "\n",
    "In practice we would want to spend more time understanding the drivers behind the outliers, and address as many as possible. Given this is an **information retrieval** promlem we want to avoid excluding valid records. \n",
    "\n",
    "However, for the purposes of this exercise we will focus on the LaTeX issue. Data tables could be solved through an application of difference pdf cracking techniques (e.g. [Azure Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-retrieval-augmented-generation?view=doc-intel-4.0.0)), but that is out of scope for this notebook.\n",
    "\n",
    "Let's remove the the lines which include LaTeX and take another look at the adjusted distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_utils.general import remove_latex_packages\n",
    "\n",
    "# Remove LaTeX package inclusions from the articles\n",
    "ds['article'] = ds['article'].apply(remove_latex_packages)\n",
    "\n",
    "# Recalculate article lengths\n",
    "ds['article_len'] = ds['article'].apply(lambda x: len(x.split()))\n",
    "\n",
    "display(ds.describe(percentiles=[0.75,0.8, 0.9,0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made a difference, but 97k is still very large. For now, let's exclude the longer documents, storing them in another dataframe for analysis later.\n",
    "\n",
    "Once we remove the odd docs, we'll check our distribution again to make sure that we now have something workable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_utils.general import remove_over_percentile\n",
    "\n",
    "#apply helper function from utils module\n",
    "ds_99pct, ds_outliers = remove_over_percentile(ds, 'article_len', .99)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first histogram on the first subplot\n",
    "sns.histplot(ds_99pct['article_len'], bins=50, kde=True, ax=axs[0])\n",
    "axs[0].set_title('Article Length')\n",
    "\n",
    "# Plot the second histogram on the second subplot\n",
    "sns.histplot(ds_99pct['abstract_len'], bins=50, kde=True, ax=axs[1])\n",
    "axs[1].set_title('Abstract Length')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(ds_99pct.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get chunking!\n",
    "\n",
    "Now we know the distribution of our document lengths, we can start to look at how to approach chunking. Remember our questions were:\n",
    "\n",
    "- What length should my chunks be?\n",
    "- Should all of my chunks be the same size?\n",
    "- What's the distribution of my chunk lengths?\n",
    "- How should I consider the relationship between my chunk length, and the context window in my generation step?\n",
    "- Do my chunks make sense in the context of my business problem?\n",
    "- Is semantic purity important for my chunks? How do I measure it?\n",
    "\n",
    "Let's start with the first question. There won't be a one size fits all answer here, but there will be \"non functional\" considerations. We now know that our mean article length is about `3000` words, and the max is ~`10,000` words. What does that tell us?\n",
    "1. the number of calls that we need to make to an embedding service will be `3000 / chunk_size'\n",
    "2. Whilst we could fit entire documents into the generation step of RAG using models with a large context window (e.g. Claude 3 - Opus) - we probably don't want to\n",
    "\n",
    "> **Note: Words and tokens**: *Despite feeling like LLMs converse in our language, there's a few things that go in behind the scenes that translate our verbiage into something an algorithm understands. Firstly, the text is `tokenized`, which means words are split into a list of `tokens`. Think of this a bit like stemming in NLP. For shorter words, the ratio of tokens to words can be 1:1 (i.e. the word = the token), but for longer, or more complex words the ratio can be far higher. These lists are then converted into numerical vectors that the algorithm can understand. For a given corpus, we could work out the exact ratio - in fact, let's do that!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tokeniser over the articles and abstracts and store the results in the DataFrame\n",
    "encoding = tk.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "article_tokens = ds_99pct['article'].apply(encoding.encode)\n",
    "abstract_tokens = ds_99pct['abstract'].apply(encoding.encode)\n",
    "\n",
    "\n",
    "# check if columns already exist\n",
    "if 'article_tokens' in ds_99pct.columns:\n",
    "    ds_99pct = ds_99pct.drop(columns=['article_tokens'])\n",
    "if 'abstract_tokens' in ds_99pct.columns:\n",
    "    ds_99pct = ds_99pct.drop(columns=['abstract_tokens'])\n",
    "\n",
    "ds_99pct = ds_99pct.assign(article_tokens=article_tokens, abstract_tokens=abstract_tokens)\n",
    "ds_99pct['article_tk_len'] = ds_99pct['article_tokens'].apply(lambda x: len(x))\n",
    "ds_99pct['abstract_tk_len'] = ds_99pct['abstract_tokens'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distributions again. You could use a histogram, but I prefer box plots, or violin pots if I'm feeling fancy. These are great as the show the univariate stats (mean/median etc.) but also provide the same \"distribution curve\" visual that you'd get from a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, median and mode for the token lengths\n",
    "article_token_mean = ds_99pct['article_tk_len'].mean()\n",
    "article_token_median = ds_99pct['article_tk_len'].median()\n",
    "article_token_mode = ds_99pct['article_tk_len'].mode()[0]\n",
    "abstract_tokens_mean = ds_99pct['abstract_tk_len'].mean()\n",
    "abstract_tokens_median = ds_99pct['abstract_tk_len'].median()\n",
    "abstract_tokens_mode = ds_99pct['abstract_tk_len'].mode()[0]\n",
    "\n",
    "print(\"***----***---***---***\")\n",
    "print(\"This is what we're working with:\\n\")\n",
    "print(f'Mean Article Tokens: {article_token_mean}')\n",
    "print(f'Median Article Tokens: {article_token_median}')\n",
    "print(f'Mode Article Tokens: {article_token_mode}')\n",
    "print(f'Mean Abstract Tokens: {abstract_tokens_mean}')\n",
    "print(f'Median Abstract Tokens: {abstract_tokens_median}')\n",
    "print(f'Mode Abstract Tokens: {abstract_tokens_mode}')\n",
    "print(\"***----***---***---***\")\n",
    "\n",
    "# Let's plot the token lengths as violin plots as two panels and call out the mean, median and mode\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the first violin plot on the first subplot\n",
    "sns.violinplot(y=ds_99pct['article_tk_len'], ax=axs[0])\n",
    "axs[0].set_title('Article Tokens')\n",
    "axs[0].axhline(article_token_mean, color='red', linestyle='--', label='Mean')\n",
    "axs[0].axhline(ds_99pct['article_tk_len'].median(), color='green', linestyle='--', label='Median')\n",
    "axs[0].axhline(ds_99pct['article_tk_len'].mode()[0], color='blue', linestyle='--', label='Mode')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the second violin plot on the second subplot\n",
    "sns.violinplot(y=ds_99pct['abstract_tk_len'], ax=axs[1])\n",
    "axs[1].set_title('Abstract Tokens')\n",
    "axs[1].axhline(abstract_tokens_mean, color='red', linestyle='--', label='Mean')\n",
    "axs[1].axhline(ds_99pct['abstract_tk_len'].median(), color='green', linestyle='--', label='Median')\n",
    "axs[1].axhline(ds_99pct['abstract_tk_len'].mode()[0], color='blue', linestyle='--', label='Mode')\n",
    "axs[1].legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with our mean article token length of 3800, and article length of 3000 - which gives us a ratio of aproximately 1.25 or 5:4 for our specific corpus. \n",
    "\n",
    "# Why did do we care about this?\n",
    "\n",
    "How many records do we want to include in our Augmentation step when constructing the generation prompt? Say we're using GPT-35-Turbo, we have aprx 4000 tokens to play with (for an explanation of tokens see [this](https://www.tokencounter.io/) excellent resource). This is both input and output.\n",
    "Let's assume we have a prompt template which is a total of 500 tokens, including our guardrails, instructions and any other boiler plate commentary that needs to be input to the generation step. Say we then allow for up to 500 tokens in a response. This leaves us with 3000 tokens (or 2400 words) to play with. If we assume a chunk size (in number of words) of 400, that gives us ~6 records in our retrieval step. In fact, this might be a good starting point. Why not try baseline chunking with with this as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now apply this to our dataset\n",
    "from topic_utils.general import chunk_string_with_overlap\n",
    "\n",
    "# Create a new DataFrame with each chunk as a separate row\n",
    "chunks = []\n",
    "doc_ids = []\n",
    "chunk_ids = []\n",
    "for idx, row in ds_99pct.iterrows():\n",
    "    article_chunks = chunk_string_with_overlap(input_text=row['article'], chunk_length=400, overlap=50)\n",
    "    chunks.extend(article_chunks)\n",
    "    doc_ids.extend([row['doc_id']] * len(article_chunks))\n",
    "    chunk_ids.extend([f\"{row['doc_id']}-{i+1}\" for i in range(len(article_chunks))])\n",
    "\n",
    "ds_chunked = pd.DataFrame({'doc_id': doc_ids, 'chunk_id': chunk_ids, 'chunks': chunks})\n",
    "\n",
    "# Worl out the average number of chunks per document\n",
    "avg_chunks_per_doc = ds_chunked.groupby('doc_id').size().mean()\n",
    "print(f'Average number of chunks per document: {avg_chunks_per_doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have some chunks - let's start having fun\n",
    "\n",
    "We have some operational concerns to deal with next. Before we can work out if the chunking strategy is any good, we will need to embed the chunks, and store them in a vector database. We'll then need to come up with some basic questions and answers to test the system - GPT4 is ideal for this. \n",
    "\n",
    "> Note: Whilst we could include a variety of search configurations to test, here we're only concerned with the relevance of the chunks compared to the question. We'll simplify the problem by simply measuring the cosine similarity of the question and answer for now.\n",
    "\n",
    "For each document, we'll use GPT4 to generate 5 questions and answers as our test set. To save on time and money, we'll reduce the number of articles we're dealing with down to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random 50 unique doc_ids and subset both the ds_99pct and new_df dataframes using these IDs\n",
    "random.seed(42)\n",
    "random_doc_ids = random.sample(list(ds_99pct['doc_id'].unique()), 50)\n",
    "\n",
    "# Subset the DataFrames\n",
    "ds_subset = ds_99pct[ds_99pct['doc_id'].isin(random_doc_ids)]\n",
    "\n",
    "ds_chunked_subset = ds_chunked[ds_chunked['doc_id'].isin(random_doc_ids)]\n",
    "\n",
    "# Submit the articles to GPT-3.5-turbo for Q&A creation \n",
    "def generate_qa_prompt(article):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following article, generate 5 Question/Answer pairs that could be used to test a student's understanding of the material:\n",
    "\n",
    "    Article:\\n\n",
    "    {article}\\n\n",
    "\n",
    "    The output should be a list of dictionaries, with each question/answer pair structured as follows:\n",
    "    {{\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris\"\n",
    "    }}\n",
    "\n",
    "    Only provide the data in as describes, do not include any other information in the output.\n",
    "    Ensure that the output is formatted as a list of dictionaries.\n",
    "    Do not include markdown or any other formatting in the output e.g. no ```json.\n",
    "    Do not generate questions that are too similar to each other.\n",
    "    Do not generate questions that require external knowledge.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "prompts = [generate_qa_prompt(article) for article in ds_subset['article']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithreaded ~ 13x faster (3mins vs 40mins)\n",
    "#TODO: Add a check for the file, if it exists read it in, else do the hit the endpoint\n",
    "\n",
    "from topic_utils.openai_utils import general_prompt, create_client\n",
    "from topic_utils.general import convert_to_dict\n",
    "\n",
    "client = create_client()\n",
    "multi_threading = True\n",
    "\n",
    "# CHeck if file already exists\n",
    "if os.path.exists('data/qa_pairs.jsonl'):\n",
    "    print(\"File exists, reading in...\")\n",
    "\n",
    "    with open('data/qa_pairs.jsonl', 'r') as f:\n",
    "        qa_pairs = json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "else:\n",
    "    # Note changing this to 3.5 without implementing guardrails may result in malformed results...\n",
    "    model = 'gpt-4'\n",
    "\n",
    "    if multi_threading == True:\n",
    "        def process_article(article):\n",
    "            return general_prompt(client, generate_qa_prompt(article), model=model)\n",
    "\n",
    "        with Pool() as pool:\n",
    "            results_multiprocessing = pool.map(process_article, ds_subset['article'])\n",
    "\n",
    "    else:\n",
    "        results = [general_prompt(client, prompt, model=model) for prompt in prompts]\n",
    "\n",
    "    # Save the results to a file\n",
    "    qa_pairs = convert_to_dict(results)\n",
    "\n",
    "    with open('data/qa_pairs.jsonl', 'w') as f:\n",
    "        json.dump(qa_pairs, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Retrieval step\n",
    "### Check in\n",
    "Where are we up to:\n",
    "- We have a good understanding of our corpus and have done some housekeeping\n",
    "- We understand tokens, and how our chunks are related to context length\n",
    "- We have a chunked data set to act as a baseline\n",
    "- We have generated some ground truth data to evaluate our chunks\n",
    "\n",
    "What do we need to do next:\n",
    "- Embed our data\n",
    "- Store it in a vector database\n",
    "- Query the db using our ground truth questions\n",
    "- Generate a final response\n",
    "- Run it through an evaluation framework\n",
    "- ITERATE\n",
    "\n",
    "Let's start with embeddings. There are many different embedding models out there. I'm going to assume that given we've used Azure Open AI, that we can also access an embedding model through the same resource. Be sure to have things configured in your `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Takes about 2 mins to run\n",
    "def generate_embeddings(text, model=\"text-embedding-ada-002\"):\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# if a column exists delete it\n",
    "if 'ada_v2' in ds_chunked_subset.columns:\n",
    "    ds_chunked_subset = ds_chunked_subset.drop(columns=['ada_v2'])\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists('data/chunked_embeddings.csv'):\n",
    "    print(\"File exists, reading in...\")\n",
    "\n",
    "    ds_chunked_subset = pd.read_csv('data/chunked_embeddings.csv')\n",
    "    ds_chunked_subset['ada_v2'] = ds_chunked_subset['ada_v2'].apply(ast.literal_eval)\n",
    "    \n",
    "else:\n",
    "\n",
    "    if multi_threading == True:\n",
    "        with Pool() as pool:\n",
    "            results_multiprocessing = pool.map(generate_embeddings, ds_chunked_subset['chunks'])\n",
    "            ds_chunked_subset['ada_v2'] = results_multiprocessing\n",
    "\n",
    "    else:\n",
    "        ds_chunked_subset['ada_v2'] = ds_chunked_subset[\"chunks\"].apply(lambda x : generate_embeddings (x, model = 'text-embedding-ada-002'))\n",
    "\n",
    "    # Save the results to a file\n",
    "    ds_chunked_subset.to_csv('data/chunked_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds_chunked_subset['ada_v2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets store these in a vector database. To keep things simple, we've elected to use ChromaDB, which is the \"SQLLite\" of the vector database world. Note, we could have skipped the above step and used ChromaDB's OpenAIEmbedding interface. We'll need to set this up to embed queries regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                api_type=\"azure\",\n",
    "                api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "chroma_client = Client()\n",
    "\n",
    "# if chroma_client.list_collections()[0].name == \"baseline_pubmed_articles\":\n",
    "#     chroma_client.delete_collection(name=\"baseline_pubmed_articles\")\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"baseline_pubmed_articles\",embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "collection.add(\n",
    "    embeddings=ds_chunked_subset['ada_v2'].tolist(),\n",
    "    documents=ds_chunked_subset['chunks'].tolist(),\n",
    "    metadatas=[{\"doc_id\": doc_id} for doc_id in ds_chunked_subset['doc_id']],\n",
    "    ids=ds_chunked_subset['chunk_id'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"What is the main diagnostic criterion related to Levodopa (LD) responsiveness in Parkinson's Disease (PD)?\"],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results object contains:\n",
    "- The chunk IDs\n",
    "- The distances (in our case, cosine similarity) between the question and the chunks\n",
    "- The metadata we injected with the prompt - in our case the doc_ids (in another notebook we can talk about *hybrid search*)\n",
    "- The embeddings are set to none by default as typically they're not particularly useful here\n",
    "- The actual chunk contents\n",
    "\n",
    "Different vector DBs will return different responses, but most will be similar to what we have here.\n",
    "\n",
    "If we were looking to tune the search params, we might pause at this step to see if there's a way to increase the relevance of the results to the query; but we'll look at that another time! For now, let's move on to using these responses to actually answer the user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augementation Step\n",
    "Now we need to create a query which takes those responses, and injects them into a final generation prompt to be submitted to an LLM. Let's try it out on a single Q&A pair first to make sure everything is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '\\n'.join(results['documents'][0])\n",
    "question = 'What is the main diagnostic criterion related to Levodopa (LD) responsiveness in Parkinson\\'s Disease (PD)?'\n",
    "\n",
    "generation_prompt = f\"\"\"\n",
    "You provide answers to questions based on information available. You give precise answers to the question asked.\n",
    "You do not answer more than what is needed. You are always exact to the point. You Answer the question using the provided context.\n",
    "If the answer is not contained within the given context, say 'I dont know.'. \n",
    "The below context is an excerpt from a report or data.\n",
    "Answer the user question using only the data provided in the sources below.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    " \n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "model = 'gpt-4'\n",
    "\n",
    "final_result = general_prompt(client, generation_prompt, model=model)\n",
    "\n",
    "pprint(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(question, client, model, collection):\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5\n",
    "    )\n",
    "\n",
    "    context = '\\n'.join(results['documents'][0])\n",
    "    generation_prompt = f\"\"\"\n",
    "    You provide answers to questions based on information available. You give precise answers to the question asked.\n",
    "    You do not answer more than what is needed. You are always exact to the point. You Answer the question using the provided context.\n",
    "    If the answer is not contained within the given context, say 'I dont know.'. \n",
    "    The below context is an excerpt from a report or data.\n",
    "    Answer the user question using only the data provided in the sources below.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "\n",
    "    QUESTION:\n",
    "    {question}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    return general_prompt(client, generation_prompt, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_rag(\"What is the main diagnostic criterion related to Levodopa (LD) responsiveness in Parkinson's Disease (PD)?\", client, model, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions and answers from qa_pairs\n",
    "questions =[pair['question'] for pair in qa_pairs]\n",
    "answers = [pair['answer'] for pair in qa_pairs]\n",
    "\n",
    "# Create a DataFrame with the questions and answers\n",
    "qa_df = pd.DataFrame({'question': questions, 'answer': answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If things are taking too long (e.g. more than 10 minutes), you can switch to the smaller model\n",
    "model = 'gpt-4' #'gpt-35-turbo-16k'\n",
    "\n",
    "if os.path.exists(f'data/qa_df-{model}.csv'):\n",
    "    print(\"File exists, reading in...\")\n",
    "\n",
    "    qa_df = pd.read_csv(f'data/qa_df-{model}.csv')\n",
    "\n",
    "else:\n",
    "    def ask_rag_wrapper(question):\n",
    "        return ask_rag(question, client, model, collection)\n",
    "\n",
    "    if multi_threading == True:\n",
    "        with Pool() as pool:\n",
    "            results_multiprocessing = pool.map(ask_rag_wrapper, qa_df['question'])\n",
    "        qa_df['mt_answer'] = results_multiprocessing\n",
    "\n",
    "    else:\n",
    "        qa_df['response'] = qa_df['question'].apply(lambda x: ask_rag(x, client, model, collection))\n",
    "\n",
    "#write out to CSV\n",
    "qa_df.to_csv(f'data/qa_df-{model}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.ai.generative.evaluate import evaluate\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "# project details\n",
    "SUBSCRIPTION_ID = \"bc6dcbb4-93d4-4ee9-8e1c-2a265ede8e06\"\n",
    "RESOURCE_GROUP = \"promptflow\"\n",
    "PROJECT_NAME = \"agromova-8777\"\n",
    "AZUREAIPROJECT_KEY = \"44027ae2fb564731b9ab5978ce1bdfd0\"\n",
    "AZUREAIPROJECT_ENDPOINT = \"https://agazureaistudi0026803630.openai.azure.com/\"\n",
    "\n",
    "\n",
    "# Create Azure AI Studio client\n",
    "ai_client = AIClient(\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    project_name=PROJECT_NAME,\n",
    "    credential=AzureCliCredential(),\n",
    ")\n",
    "# Get the default Azure Open AI connection for your project\n",
    "ai_client.get_default_aoai_connection().set_current_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate( \n",
    "    evaluation_name=\"my-qa-eval-with-data\", #name your evaluation to view in AI Studio\n",
    "    data=qa_df, # data to be evaluated\n",
    "    task_type=\"qa\",\n",
    "    metrics_list=[\"gpt_groundedness\",\"gpt_relevance\",\"gpt_coherence\",\"gpt_fluency\",\"gpt_similarity\",\"hate_unfairness\",\"sexual\",\"violence\",\"self_harm\"],\n",
    "    model_config= { #for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement\n",
    "            \"api_version\": \"2023-05-15\",\n",
    "            \"api_base\": AZUREAIPROJECT_ENDPOINT,\n",
    "            \"api_type\": \"azure\",\n",
    "            \"api_key\": AZUREAIPROJECT_KEY,\n",
    "            \"deployment_id\": \"gpt-4\"\n",
    "    },\n",
    "    data_mapping={\n",
    "        \"question\":\"question\", #column of data providing input to model\n",
    "        \"context\":\"context\", #column of data providing context for each input\n",
    "        \"answer\":\"answer\", #column of data providing output from model\n",
    "        \"ground_truth\":\"groundtruth\" #column of data providing ground truth answer, optional for default metrics\n",
    "        },\n",
    "    output_path=\"./myevalresults\", #optional: save evaluation results .jsonl to local folder path \n",
    "    tracking_uri=client.tracking_uri #optional: if configured with AI client, evaluation gets logged to AI Studio\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
